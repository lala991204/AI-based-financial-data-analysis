{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zKPxdTstUk3_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import umap\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers, constraints, regularizers\n",
    "from tensorflow.keras.layers import Input, Layer, Lambda, Dropout, Reshape, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knl8dzhvrgAk"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyDpI22NVs3N"
   },
   "source": [
    "## Read in edges, features, and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 유저 데이터\n",
    "old_df= pd.read_csv('MLRD_UserData_20220131.csv')\n",
    "# 새로운 유저 데이터\n",
    "new_df = pd.read_csv('MLRD_UserData_20220227.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "## 현재 new_df에서 피쳐 데이터로 쓰이는 kyc_level부분의 type을 object->int 수정해주었습니다.(ex: value가 0과 '0'이 다른 것으로 구분되므로)\n",
    "new_df['max_kyc_level'] = new_df['max_kyc_level'].replace('unknown', 0)\n",
    "new_df['max_kyc_level'] = pd.to_numeric(new_df['max_kyc_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sort = old_df.sort_values(by='signup_time')         \n",
    "old_sort1 = old_sort.reset_index()\n",
    "old_sort2 = old_sort1.drop('index', axis=1)\n",
    "\n",
    "new_sort = new_df.sort_values(by='signup_time')         \n",
    "new_sort1 = new_sort.reset_index()\n",
    "new_sort2 = new_sort1.drop('index', axis=1)\n",
    "\n",
    "old1 = old_sort2[['user_id', 'no_of_days_since_last_trade', 'total_no_of_trade_times', 'accumulative_trading_amount_in_usdt',\n",
    "             'signup_channel_type', 'signup_device_type', 'signup_country_en', 'max_kyc_level', 'first_deposit_method', 'signup_channel_id']]\n",
    "\n",
    "new1 = new_sort2[['user_id', 'no_of_days_since_last_trade', 'total_no_of_trade_times', 'accumulative_trading_amount_in_usdt',\n",
    "             'signup_channel_type', 'signup_device_type', 'signup_country_en', 'max_kyc_level', 'first_deposit_method', 'signup_channel_id']]\n",
    "\n",
    "\n",
    "# 세 개의 컬럼에 대해 결측치 행 모두 제거\n",
    "old3 =old1.dropna(axis=0)\n",
    "old4 = old3.reset_index()\n",
    "old5 = old4.drop('index', axis=1)\n",
    "\n",
    "orfm_df2 = pd.DataFrame()\n",
    "orfm_df2['CustomerID'] = old5['user_id']\n",
    "orfm_df2['Recency'] = old5['no_of_days_since_last_trade']            # 최근 거래 후 몇일 지났는지\n",
    "orfm_df2['Frequency'] = old5['total_no_of_trade_times']              # 거래 총 수\n",
    "orfm_df2['Monetary'] = old5['accumulative_trading_amount_in_usdt']   # 총 거래 금액(USDT)\n",
    "\n",
    "orfm_df1 = orfm_df2.reset_index()\n",
    "orfm_df = orfm_df1.drop('index', axis=1)\n",
    "\n",
    "### Log 변환을 통해 데이터 변환\n",
    "\n",
    "# Recency, Frequecny, Monetary 컬럼에 np.log1p() 로 Log Transformation\n",
    "orfm_df['Recency_log'] = np.log1p(orfm_df['Recency'])\n",
    "orfm_df['Frequency_log'] = np.log1p(orfm_df['Frequency'])\n",
    "orfm_df['Monetary_log'] = np.log1p(orfm_df['Monetary'])\n",
    "\n",
    "# Log Transformation 데이터에 StandardScaler 적용\n",
    "X_features = orfm_df[['Recency_log','Frequency_log','Monetary_log']].values\n",
    "X_features_scaled = StandardScaler().fit_transform(X_features)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "labels = kmeans.fit_predict(X_features_scaled)\n",
    "orfm_df['cluster_label'] = labels\n",
    "\n",
    "old5['cluster'] = orfm_df['cluster_label']\n",
    "\n",
    "df = pd.concat([old5, new1])   # 둘이 날짜형식 안 맞아서 따로 정렬 후 병합/ old부분만  target을 제대로 만들고  new는 모두 0처리 할 것임\n",
    "df['cluster'].replace(np.nan,0, inplace=True)\n",
    "df6 = df[(df['cluster']==0) | (df['cluster']==2)]\n",
    "\n",
    "df6['cluster'] = df6['cluster'].replace(2,1)\n",
    "df7 = df6.reset_index()\n",
    "df8 = df7.drop('index', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# 가입 경로 유형에 따라 엣지 데이터 생성\n",
    "arr_org = np.array(df8[df8['signup_channel_type']=='Organic traffic'].index)\n",
    "arr_ref = np.array(df8[df8['signup_channel_type']=='Referral'].index)\n",
    "arr_seo = np.array(df8[df8['signup_channel_type']=='SEO'].index)\n",
    "arr_aff = np.array(df8[df8['signup_channel_type']=='Affiliate'].index)\n",
    "arr_cam = np.array(df8[df8['signup_channel_type']=='Campaign promotion'].index)\n",
    "arr_oth = np.array(df8[df8['signup_channel_type']=='Others'].index)\n",
    "\n",
    "edges_df = pd.DataFrame()\n",
    "start=[]\n",
    "end=[]\n",
    "arrs=[arr_org, arr_ref, arr_seo, arr_aff, arr_cam, arr_oth]\n",
    "\n",
    "for arr in tqdm(arrs):\n",
    "    for i in range(len(arr)-1):\n",
    "        for j in range(i+1, len(arr)):\n",
    "            start.append(arr[i])\n",
    "            end.append(arr[j])\n",
    "\n",
    "edges_df['source'] = start\n",
    "edges_df['target'] = end        \n",
    "\n",
    "\n",
    "\n",
    "# 피쳐 데이터 생성\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "f1 = pd.get_dummies(df8['signup_device_type'])\n",
    "f2 = pd.get_dummies(df8['signup_country_en'])\n",
    "f3 = pd.get_dummies(df8['max_kyc_level'])\n",
    "f4 = pd.get_dummies(df8['first_deposit_method'])\n",
    "\n",
    "i=0\n",
    "fs=[f1,f2,f3,f4]\n",
    "for f in tqdm(fs):\n",
    "    for col in f.columns:\n",
    "        features_df[i] = f[col]\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "# 타겟 데이터\n",
    "targets_df1 = pd.DataFrame()\n",
    "targets_df1['id'] = df8['user_id']\n",
    "targets_df1['target'] = df8['cluster']\n",
    "targets_df2 = targets_df1.reset_index()\n",
    "targets_df = targets_df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8SyB5qErVZB"
   },
   "source": [
    "## Train/Test/Val split & Preprocessing & GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "id": "9cYXxuDLqkPh"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect() \n",
    "\n",
    "G = sg.StellarGraph(features_df, edges_df)\n",
    "\n",
    "t1 = round((len(targets_df)-len(new_df))*(6/10))\n",
    "t2 = ((len(targets_df)-len(new_df)) - t1\n",
    "train_pages1, test_pages1 = train_test_split(targets_df, train_size=t1, shuffle=False, random_state=45)\n",
    "val_pages1, test_pages1 = train_test_split(test_pages1, train_size=t2, shuffle=False, random_state=120) \n",
    "      \n",
    "target_encoding = LabelBinarizer()  # 2개의 클래스가 있을 때, 이에 대한 벡터를 반환함\n",
    "train_targets1 = target_encoding.fit_transform(train_pages1['target'])\n",
    "val_targets1 = target_encoding.transform(val_pages1['target'])\n",
    "test_targets1 = target_encoding.transform(test_pages1['target'])\n",
    "\n",
    "      \n",
    "# Initialize the generator\n",
    "generator = FullBatchNodeGenerator(G, method=\"gcn\")\n",
    "\n",
    "# Use the .flow method to prepare it for use with GCN\n",
    "train_gen1 = generator.flow(train_pages1.index, train_targets1)\n",
    "val_gen1 = generator.flow(val_pages1.index, val_targets1)\n",
    "test_gen1 = generator.flow(test_pages1.index, test_targets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build necessary layers\n",
    "gcn = GCN(\n",
    "    layer_sizes=[32,32], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
    "    )\n",
    "\n",
    "# Access the input and output tensors\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# Pass the output tensor through the dense layer with sigmoid\n",
    "predictions = layers.Dense(units=train_targets1.shape[1], activation=\"sigmoid\")(x_out)\n",
    "\n",
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adam(lr=0.01),\n",
    "    loss = losses.binary_crossentropy,\n",
    "    metrics = [\"acc\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen1,\n",
    "    epochs = 200,\n",
    "    validation_data = val_gen1,\n",
    "    verbose = 1,\n",
    "    shuffle=False,         \n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_preds(true, pred):\n",
    "    auc = roc_auc_score(true, pred)\n",
    "    pr = average_precision_score(true, pred)\n",
    "    bin_pred = [1 if p > 0.5 else 0 for p in pred]   # 0.5\n",
    "    f_score = f1_score(true, bin_pred)\n",
    "    print('ROC AUC:', auc)\n",
    "    print('PR AUC:', pr)\n",
    "    print('F1 score:', f_score)\n",
    "    print(confusion_matrix(true, bin_pred, normalize='true'))\n",
    "\n",
    "    return auc, pr, f_score\n",
    "\n",
    "\n",
    "new_preds1 = model.predict(test_gen1)\n",
    "new_preds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rk4uCldLWlB"
   },
   "source": [
    "#  신규고객 세그먼테이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_df.copy()\n",
    "bin_pred = [1 if p > 0.5 else 0 for p in new_preds1[0].ravel()] \n",
    "result['pred_cluster'] = bin_pred\n",
    "\n",
    "result0 = result[result['pred_cluster']==0]\n",
    "result1 = result[result['pred_cluster']==1]\n",
    "\n",
    "\n",
    "# 군집0 추출\n",
    "result0['user_id'].to_csv('group0.csv')\n",
    "\n",
    "# 군집1 추출\n",
    "result1['user_id'].to_csv('group1.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GitHub_User_Classification_using_GCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
